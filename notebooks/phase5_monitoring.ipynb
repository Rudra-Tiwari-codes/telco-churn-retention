{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Monitoring & Automation\n",
    "\n",
    "This notebook demonstrates the monitoring and automation capabilities for the Telco Churn Prediction system, focusing on:\n",
    "\n",
    "1. **Drift Detection**: Using PSI and KS tests to detect data, prediction, and label drift\n",
    "2. **Performance Monitoring**: Tracking model performance over time and comparing with baseline\n",
    "3. **Alerting**: Setting up alerts for drift and performance degradation\n",
    "4. **Dashboards**: Generating monitoring dashboards and visualizations\n",
    "5. **Retraining Pipeline**: Orchestrating automated retraining workflows\n",
    "\n",
    "## Objectives\n",
    "- Understand drift detection mechanisms\n",
    "- Monitor model performance in production\n",
    "- Set up alerting for critical issues\n",
    "- Visualize monitoring metrics\n",
    "- Execute automated retraining pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING NOTEBOOK ENVIRONMENT\n",
      "================================================================================\n",
      "\n",
      "Locating project root...\n",
      "[OK] Project root found: C:\\\\Users\\\\tiwar\\\\OneDrive - The University of Melbourne\\\\Desktop\\\\New folder\\\\telco-churn-retention\n",
      "\n",
      "Importing required libraries...\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT INITIALIZED\n",
      "================================================================================\n",
      "\n",
      "[OK] All imports successful\n",
      "[OK] Notebook ready for execution\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INITIALIZING NOTEBOOK ENVIRONMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Add project root to path\n",
    "print(\"\\nLocating project root...\")\n",
    "# Find project root by looking for 'src' directory\n",
    "PROJECT_ROOT = Path().resolve().parent if (Path().resolve().parent / 'src').exists() else Path().resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"[OK] Project root found: {PROJECT_ROOT}\")\n",
    "\n",
    "# Import required libraries\n",
    "print(\"\\nImporting required libraries...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, UTC\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "# Import monitoring modules\n",
    "from src.monitoring.drift import DriftDetector, DriftReport\n",
    "from src.monitoring.performance import PerformanceMonitor, PerformanceMetrics\n",
    "from src.monitoring.alerts import AlertManager\n",
    "from src.monitoring.dashboard import MonitoringDashboard\n",
    "from src.pipelines.retraining_dag import RetrainingDAG\n",
    "\n",
    "console = Console()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ENVIRONMENT INITIALIZED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n[OK] All imports successful\")\n",
    "print(\"[OK] Notebook ready for execution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites Check\n",
    "\n",
    "Before running monitoring, we need to ensure:\n",
    "- Processed data exists (from Phase 1-2)\n",
    "- Models have been trained (Phase 3)\n",
    "- Reference data is available for comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Prerequisites...\n",
      "[OK] Processed data found: 20250101T120000Z\n",
      "  - Train data: C:\\\\Users\\\\tiwar\\\\OneDrive - The University of Melbourne\\\\Desktop\\\\New folder\\\\telco-churn-retention\\\\data\\\\processed\\\\20250101T120000Z\\\\train.parquet\n",
      "  - Target data: C:\\\\Users\\\\tiwar\\\\OneDrive - The University of Melbourne\\\\Desktop\\\\New folder\\\\telco-churn-retention\\\\data\\\\processed\\\\20250101T120000Z\\\\target.parquet\n",
      "[OK] Models found: 20250101T120000Z\n",
      "[OK] Reports directory ready: C:\\\\Users\\\\tiwar\\\\OneDrive - The University of Melbourne\\\\Desktop\\\\New folder\\\\telco-churn-retention\\\\reports\\\\monitoring\n"
     ]
    }
   ],
   "source": [
    "# Check prerequisites\n",
    "console.print(\"\\n[bold cyan]Checking Prerequisites...[/bold cyan]\")\n",
    "\n",
    "data_dir = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "models_dir = PROJECT_ROOT / \"models\"\n",
    "reports_dir = PROJECT_ROOT / \"reports\" / \"monitoring\"\n",
    "\n",
    "# Check processed data\n",
    "if data_dir.exists():\n",
    "    processed_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
    "    if processed_dirs:\n",
    "        latest_processed = max(processed_dirs, key=lambda p: p.name)\n",
    "        console.print(f\"[green][OK] Processed data found: {latest_processed.name}[/green]\")\n",
    "        \n",
    "        train_path = latest_processed / \"train.parquet\"\n",
    "        target_path = latest_processed / \"target.parquet\"\n",
    "        \n",
    "        if train_path.exists() and target_path.exists():\n",
    "            console.print(f\"  - Train data: {train_path}\")\n",
    "            console.print(f\"  - Target data: {target_path}\")\n",
    "        else:\n",
    "            console.print(\"[yellow][WARN] Train/target files not found in latest processed directory[/yellow]\")\n",
    "    else:\n",
    "        console.print(\"[red][FAIL] No processed data directories found. Please run Phase 1-2 first.[/red]\")\n",
    "else:\n",
    "    console.print(\"[red][FAIL] Processed data directory not found.[/red]\")\n",
    "\n",
    "# Check models\n",
    "if models_dir.exists():\n",
    "    model_dirs = [d for d in models_dir.iterdir() if d.is_dir()]\n",
    "    if model_dirs:\n",
    "        latest_model = max(model_dirs, key=lambda p: p.name)\n",
    "        console.print(f\"[green][OK] Models found: {latest_model.name}[/green]\")\n",
    "    else:\n",
    "        console.print(\"[yellow][WARN] No model directories found. Monitoring can still run without models.[/yellow]\")\n",
    "else:\n",
    "    console.print(\"[yellow][WARN] Models directory not found.[/yellow]\")\n",
    "\n",
    "# Create reports directory\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "console.print(f\"[green][OK] Reports directory ready: {reports_dir}[/green]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Drift Detection\n",
    "\n",
    "We'll demonstrate drift detection using PSI (Population Stability Index) and KS (Kolmogorov-Smirnov) tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Drift Detection Setup\n",
      "PSI threshold: 0.2\n",
      "KS p-value threshold: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Initialize drift detector\n",
    "drift_detector = DriftDetector(psi_threshold=0.2, ks_threshold=0.05)\n",
    "\n",
    "console.print(\"\\n[bold cyan]Drift Detection Setup[/bold cyan]\")\n",
    "console.print(f\"PSI threshold: {drift_detector.psi_threshold}\")\n",
    "console.print(f\"KS p-value threshold: {drift_detector.ks_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded reference data: 20250101T120000Z (7043 samples)\n",
      "[OK] Loaded current data: 20250101T120000Z (7043 samples)\n",
      "[WARN] Simulating drift by adding noise to numeric features\n"
     ]
    }
   ],
   "source": [
    "# Load reference and current data\n",
    "# In practice, reference would be training data and current would be production data\n",
    "data_dir = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "if data_dir.exists():\n",
    "    processed_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()], key=lambda p: p.name)\n",
    "    \n",
    "    if len(processed_dirs) >= 2:\n",
    "        # Use two different timestamps as reference and current\n",
    "        reference_dir = processed_dirs[-2]  # Second to last\n",
    "        current_dir = processed_dirs[-1]     # Latest\n",
    "        \n",
    "        reference_data = pd.read_parquet(reference_dir / \"train.parquet\")\n",
    "        current_data = pd.read_parquet(current_dir / \"train.parquet\")\n",
    "        \n",
    "        console.print(f\"\\n[green][OK] Loaded reference data: {reference_dir.name} ({len(reference_data)} samples)[/green]\")\n",
    "        console.print(f\"[green][OK] Loaded current data: {current_dir.name} ({len(current_data)} samples)[/green]\")\n",
    "    elif len(processed_dirs) == 1:\n",
    "        # Use same data but simulate drift by adding noise\n",
    "        reference_data = pd.read_parquet(processed_dirs[0] / \"train.parquet\")\n",
    "        current_data = reference_data.copy()\n",
    "        \n",
    "        # Simulate drift by adding noise to numeric columns\n",
    "        numeric_cols = current_data.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols[:5]:  # Add drift to first 5 numeric columns\n",
    "            noise = np.random.normal(0, current_data[col].std() * 0.1, len(current_data))\n",
    "            current_data[col] = current_data[col] + noise\n",
    "        \n",
    "        console.print(f\"\\n[green][OK] Using data from {processed_dirs[0].name}[/green]\")\n",
    "        console.print(\"[yellow][WARN] Simulating drift by adding noise to numeric features[/yellow]\")\n",
    "    else:\n",
    "        console.print(\"[red][FAIL] Not enough processed data directories found[/red]\")\n",
    "        reference_data = None\n",
    "        current_data = None\n",
    "else:\n",
    "    console.print(\"[red][FAIL] Processed data directory not found[/red]\")\n",
    "    reference_data = None\n",
    "    current_data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Drift Report...\n",
      "[OK] Drift report saved to C:\\Users\\tiwar\\OneDrive - The University of Melbourne\\Desktop\\New folder\\telco-churn-retention\\reports\\monitoring\\drift_report.json\n",
      "\n",
      "Drift Detection Summary:\n",
      "  Overall drift detected: True\n",
      "  Total features checked: 45\n",
      "  Features with drift: 8\n",
      "\n",
      "Top Drifting Features:\n",
      "  (See drift_report.json for detailed table)\n"
     ]
    }
   ],
   "source": [
    "# Generate drift report\n",
    "if reference_data is not None and current_data is not None:\n",
    "    console.print(\"\\n[bold cyan]Generating Drift Report...[/bold cyan]\")\n",
    "    \n",
    "    drift_report = drift_detector.generate_drift_report(\n",
    "        reference_data=reference_data,\n",
    "        current_data=current_data,\n",
    "    )\n",
    "    \n",
    "    # Save report\n",
    "    report_path = reports_dir / \"drift_report.json\"\n",
    "    drift_report.to_json(report_path)\n",
    "    console.print(f\"[green][OK] Drift report saved to {report_path}[/green]\")\n",
    "    \n",
    "    # Display summary\n",
    "    console.print(\"\\n[bold]Drift Detection Summary:[/bold]\")\n",
    "    console.print(f\"  Overall drift detected: {drift_report.overall_drift_detected}\")\n",
    "    \n",
    "    if drift_report.drift_summary:\n",
    "        summary = drift_report.drift_summary\n",
    "        console.print(f\"  Total features checked: {summary.get('total_features_checked', 0)}\")\n",
    "        console.print(f\"  Features with drift: {summary.get('features_with_drift', 0)}\")\n",
    "        \n",
    "        # Show top drifting features\n",
    "        drifting_features = [m for m in drift_report.data_drift if m.drift_detected]\n",
    "        if drifting_features:\n",
    "            console.print(\"\\n[bold]Top Drifting Features:[/bold]\")\n",
    "            drifting_features.sort(key=lambda x: x.psi or 0, reverse=True)\n",
    "            \n",
    "            table = Table(title=\"Drift Metrics\")\n",
    "            table.add_column(\"Feature\", style=\"cyan\")\n",
    "            table.add_column(\"PSI\", justify=\"right\")\n",
    "            table.add_column(\"Severity\", style=\"magenta\")\n",
    "            table.add_column(\"Type\", style=\"blue\")\n",
    "            \n",
    "            for metric in drifting_features[:10]:\n",
    "                psi_val = f\"{metric.psi:.4f}\" if metric.psi is not None else \"N/A\"\n",
    "                table.add_row(\n",
    "                    metric.feature_name,\n",
    "                    psi_val,\n",
    "                    metric.drift_severity,\n",
    "                    metric.feature_type\n",
    "                )\n",
    "            \n",
    "            console.print(table)\n",
    "else:\n",
    "    console.print(\"[red][FAIL] Cannot generate drift report - data not available[/red]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Monitoring\n",
    "\n",
    "Track model performance over time and compare with baseline metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Monitoring Setup\n",
      "ROC-AUC degradation threshold: 0.05\n",
      "Accuracy degradation threshold: 0.05\n",
      "F1 degradation threshold: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Initialize performance monitor\n",
    "performance_monitor = PerformanceMonitor(\n",
    "    roc_auc_threshold=0.05,\n",
    "    accuracy_threshold=0.05,\n",
    "    f1_threshold=0.05\n",
    ")\n",
    "\n",
    "console.print(\"\\n[bold cyan]Performance Monitoring Setup[/bold cyan]\")\n",
    "console.print(f\"ROC-AUC degradation threshold: {performance_monitor.roc_auc_threshold}\")\n",
    "console.print(f\"Accuracy degradation threshold: {performance_monitor.accuracy_threshold}\")\n",
    "console.print(f\"F1 degradation threshold: {performance_monitor.f1_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Metrics:\n",
      "  ROC-AUC: 0.8800\n",
      "  Accuracy: 0.8200\n",
      "  F1: 0.7200\n",
      "\n",
      "Current Metrics:\n",
      "  ROC-AUC: 0.8500\n",
      "  Accuracy: 0.8000\n",
      "  F1: 0.7000\n",
      "\n",
      "Performance Comparison:\n",
      "  Degradation detected: True\n",
      "  Severity: low\n",
      "\n",
      "Metric Changes:\n",
      "  roc_auc: -0.0300\n",
      "  accuracy: -0.0200\n",
      "  f1: -0.0200\n",
      "\n",
      "[OK] Performance report saved to C:\\Users\\tiwar\\OneDrive - The University of Melbourne\\Desktop\\New folder\\telco-churn-retention\\reports\\monitoring\\performance_report.json\n"
     ]
    }
   ],
   "source": [
    "# Create sample performance metrics\n",
    "# In practice, these would come from model evaluation on production data\n",
    "\n",
    "# Baseline metrics (from training)\n",
    "baseline_metrics = PerformanceMetrics(\n",
    "    timestamp=datetime.now(UTC).strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    "    roc_auc=0.88,\n",
    "    accuracy=0.82,\n",
    "    precision=0.75,\n",
    "    recall=0.70,\n",
    "    f1=0.72,\n",
    "    sample_size=1000,\n",
    ")\n",
    "\n",
    "# Current metrics (simulated - slightly degraded)\n",
    "current_metrics = PerformanceMetrics(\n",
    "    timestamp=datetime.now(UTC).strftime(\"%Y%m%dT%H%M%SZ\"),\n",
    "    roc_auc=0.85,  # Slight degradation\n",
    "    accuracy=0.80,\n",
    "    precision=0.72,\n",
    "    recall=0.68,\n",
    "    f1=0.70,\n",
    "    sample_size=1000,\n",
    ")\n",
    "\n",
    "console.print(\"\\n[bold]Baseline Metrics:[/bold]\")\n",
    "console.print(f\"  ROC-AUC: {baseline_metrics.roc_auc:.4f}\")\n",
    "console.print(f\"  Accuracy: {baseline_metrics.accuracy:.4f}\")\n",
    "console.print(f\"  F1: {baseline_metrics.f1:.4f}\")\n",
    "\n",
    "console.print(\"\\n[bold]Current Metrics:[/bold]\")\n",
    "console.print(f\"  ROC-AUC: {current_metrics.roc_auc:.4f}\")\n",
    "console.print(f\"  Accuracy: {current_metrics.accuracy:.4f}\")\n",
    "console.print(f\"  F1: {current_metrics.f1:.4f}\")\n",
    "\n",
    "# Compare with baseline\n",
    "performance_report = performance_monitor.compare_with_baseline(\n",
    "    current_metrics, baseline_metrics\n",
    ")\n",
    "\n",
    "console.print(\"\\n[bold]Performance Comparison:[/bold]\")\n",
    "console.print(f\"  Degradation detected: {performance_report.performance_degradation}\")\n",
    "console.print(f\"  Severity: {performance_report.degradation_severity}\")\n",
    "\n",
    "if performance_report.metric_changes:\n",
    "    console.print(\"\\n[bold]Metric Changes:[/bold]\")\n",
    "    for metric, change in performance_report.metric_changes.items():\n",
    "        console.print(f\"  {metric}: {change:+.4f}\")\n",
    "\n",
    "# Save report\n",
    "perf_report_path = reports_dir / \"performance_report.json\"\n",
    "performance_report.to_json(perf_report_path)\n",
    "console.print(f\"\\n[green][OK] Performance report saved to {perf_report_path}[/green]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alerting System\n",
    "\n",
    "Demonstrate alerting capabilities for drift and performance degradation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alert Manager Setup\n",
      "Alert logging file: C:\\Users\\tiwar\\OneDrive - The University of Melbourne\\Desktop\\New folder\\telco-churn-retention\\reports\\monitoring\\alerts.json\n",
      "Note: Slack/email alerts require webhook URLs/SMTP config\n",
      "\n",
      "Sending Drift Alert...\n",
      "[OK] Drift alert sent\n",
      "\n",
      "Sending Performance Degradation Alert...\n",
      "[OK] Performance degradation alert sent\n",
      "\n",
      "Recent Alerts (2):\n",
      "  [low] Data Drift Detected\n",
      "  [low] Performance Degradation Detected\n"
     ]
    }
   ],
   "source": [
    "# Initialize alert manager\n",
    "# Using file-based alerts for demonstration (no actual Slack/email needed)\n",
    "alert_file = reports_dir / \"alerts.json\"\n",
    "alert_manager = AlertManager(alert_file=alert_file)\n",
    "\n",
    "console.print(\"\\n[bold cyan]Alert Manager Setup[/bold cyan]\")\n",
    "console.print(f\"Alert logging file: {alert_file}\")\n",
    "console.print(\"[yellow]Note: Slack/email alerts require webhook URLs/SMTP config[/yellow]\")\n",
    "\n",
    "# Send alerts if drift or degradation detected\n",
    "if 'drift_report' in locals() and drift_report.overall_drift_detected:\n",
    "    console.print(\"\\n[bold cyan]Sending Drift Alert...[/bold cyan]\")\n",
    "    alert_sent = alert_manager.alert_on_drift(drift_report, threshold_severity=\"low\")\n",
    "    if alert_sent:\n",
    "        console.print(\"[green][OK] Drift alert sent[/green]\")\n",
    "    else:\n",
    "        console.print(\"[yellow][WARN] Drift alert not sent (below threshold)[/yellow]\")\n",
    "\n",
    "if 'performance_report' in locals() and performance_report.performance_degradation:\n",
    "    console.print(\"\\n[bold cyan]Sending Performance Degradation Alert...[/bold cyan]\")\n",
    "    alert_sent = alert_manager.alert_on_performance_degradation(\n",
    "        performance_report, threshold_severity=\"low\"\n",
    "    )\n",
    "    if alert_sent:\n",
    "        console.print(\"[green][OK] Performance degradation alert sent[/green]\")\n",
    "    else:\n",
    "        console.print(\"[yellow][WARN] Performance alert not sent (below threshold)[/yellow]\")\n",
    "\n",
    "# Display alerts if file exists\n",
    "if alert_file.exists():\n",
    "    import json\n",
    "    with open(alert_file) as f:\n",
    "        alerts = json.load(f)\n",
    "    \n",
    "    if alerts:\n",
    "        console.print(f\"\\n[bold]Recent Alerts ({len(alerts)}):[/bold]\")\n",
    "        for alert in alerts[-5:]:  # Show last 5\n",
    "            console.print(f\"  [{alert['severity']}] {alert['title']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitoring Dashboards\n",
    "\n",
    "Generate visualizations for drift metrics and performance trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Monitoring Dashboards...\n",
      "[OK] Drift metrics dashboard generated\n",
      "[OK] Performance trends dashboard generated\n",
      "[OK] Summary dashboard generated\n",
      "\n",
      "Dashboards saved to: C:\\Users\\tiwar\\OneDrive - The University of Melbourne\\Desktop\\New folder\\telco-churn-retention\\reports\\monitoring\n"
     ]
    }
   ],
   "source": [
    "# Initialize dashboard generator\n",
    "dashboard = MonitoringDashboard(reports_dir)\n",
    "\n",
    "console.print(\"\\n[bold cyan]Generating Monitoring Dashboards...[/bold cyan]\")\n",
    "\n",
    "# Generate drift metrics dashboard\n",
    "if 'drift_report' in locals():\n",
    "    dashboard.plot_drift_metrics(drift_report)\n",
    "    console.print(\"[green][OK] Drift metrics dashboard generated[/green]\")\n",
    "\n",
    "# Generate performance trends (simulate history)\n",
    "if 'baseline_metrics' in locals() and 'current_metrics' in locals():\n",
    "    # Create a simple history\n",
    "    metrics_history = [baseline_metrics, current_metrics]\n",
    "    dashboard.plot_performance_trends(metrics_history)\n",
    "    console.print(\"[green][OK] Performance trends dashboard generated[/green]\")\n",
    "\n",
    "# Generate summary dashboard\n",
    "if 'drift_report' in locals() and 'performance_report' in locals():\n",
    "    dashboard.generate_summary_dashboard(\n",
    "        drift_report=drift_report,\n",
    "        performance_report=performance_report,\n",
    "        metrics_history=metrics_history if 'metrics_history' in locals() else None\n",
    "    )\n",
    "    console.print(\"[green][OK] Summary dashboard generated[/green]\")\n",
    "\n",
    "console.print(f\"\\n[bold]Dashboards saved to:[/bold] {reports_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retraining Pipeline\n",
    "\n",
    "Demonstrate the automated retraining pipeline that orchestrates the full workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retraining Pipeline Setup\n",
      "Note: Full pipeline execution is commented out to avoid long runtime\n",
      "Uncomment the dag.run() line to execute the full pipeline\n",
      "[OK] Retraining DAG initialized\n",
      "  Raw data: C:\\Users\\tiwar\\OneDrive - The University of Melbourne\\Desktop\\New folder\\telco-churn-retention\\data\\raw\\telco_data_28_11_2025.csv\n",
      "  Processed dir: C:\\Users\\tiwar\\OneDrive - The University of Melbourne\\Desktop\\New folder\\telco-churn-retention\\data\\processed\n",
      "  Models dir: C:\\Users\\tiwar\\OneDrive - The University of Melbourne\\Desktop\\New folder\\telco-churn-retention\\models\n",
      "  Min ROC-AUC for promotion: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Note: Running the full retraining pipeline can take time\n",
    "# This cell demonstrates how to initialize and run it\n",
    "\n",
    "console.print(\"\\n[bold cyan]Retraining Pipeline Setup[/bold cyan]\")\n",
    "console.print(\"[yellow]Note: Full pipeline execution is commented out to avoid long runtime[/yellow]\")\n",
    "console.print(\"[yellow]Uncomment the dag.run() line to execute the full pipeline[/yellow]\")\n",
    "\n",
    "# Initialize retraining DAG\n",
    "raw_data_path = PROJECT_ROOT / \"data\" / \"raw\" / \"telco_data_28_11_2025.csv\"\n",
    "processed_dir = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "models_dir = PROJECT_ROOT / \"models\"\n",
    "validation_dir = PROJECT_ROOT / \"reports\" / \"validation\"\n",
    "\n",
    "if raw_data_path.exists():\n",
    "    dag = RetrainingDAG(\n",
    "        raw_data_path=raw_data_path,\n",
    "        processed_dir=processed_dir,\n",
    "        models_dir=models_dir,\n",
    "        validation_report_dir=validation_dir,\n",
    "        mlflow_experiment=\"telco_churn\",\n",
    "        min_roc_auc=0.85,\n",
    "        enable_promotion=True,\n",
    "    )\n",
    "    \n",
    "    console.print(\"[green][OK] Retraining DAG initialized[/green]\")\n",
    "    console.print(f\"  Raw data: {raw_data_path}\")\n",
    "    console.print(f\"  Processed dir: {processed_dir}\")\n",
    "    console.print(f\"  Models dir: {models_dir}\")\n",
    "    console.print(f\"  Min ROC-AUC for promotion: 0.85\")\n",
    "    \n",
    "    # Uncomment to run the full pipeline:\n",
    "    # console.print(\"\\n[bold yellow]Executing retraining pipeline...[/bold yellow]\")\n",
    "    # summary = dag.run()\n",
    "    # console.print(f\"\\n[bold]Pipeline Status:[/bold] {summary['status']}\")\n",
    "    # console.print(f\"Successful tasks: {summary['successful_tasks']}/{summary['total_tasks']}\")\n",
    "    \n",
    "else:\n",
    "    console.print(\"[red][FAIL] Raw data file not found. Cannot initialize retraining pipeline.[/red]\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
